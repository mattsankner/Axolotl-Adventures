{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c31760e9-725f-444e-b458-fedc58518b7f",
   "metadata": {},
   "source": [
    "# **Basic Training: Fine-Tuning an LLM with Axolotl**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b940edaa-3091-4af8-a3d6-42cac90d00f9",
   "metadata": {},
   "source": [
    "Follow along for a basic run down of running your first fine-tune with Axolotl!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae54bfc6-ac78-41d9-9188-d409e6a27d3e",
   "metadata": {},
   "source": [
    "## **Resources Used:**\n",
    "- [Axolotl GitHub Repo](https://github.com/OpenAccess-AI-Collective/axolotl) \n",
    "- [Hamel's Blog](https://hamel.dev/blog/posts/axolotl/) (Debugging)\n",
    "- [Axolotl Docs](https://openaccess-ai-collective.github.io/axolotl/docs/debugging.html#general-tips) (Debugging)\n",
    "- [jarvislabs Axolotl QuickStart](https://jarvislabs.ai/templates/axolotl) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2035708c-addc-4a74-89b0-5ab53c7ca711",
   "metadata": {},
   "source": [
    "## **General Workflow:**\n",
    "- Select our Framework (Axolotl)\n",
    "- Pick a model to use in the examples folder on Axolotl's Github repository\n",
    "- Pick the dataset you want the model to train on\n",
    "- Create a GPU instance with enough storage for training\n",
    "- Launch the GPU instance in the Cloud\n",
    "- Clone the repo, cd into it, create a venv\n",
    "- Download dependencies\n",
    "- Authentication command: some models in HuggingFace require this\n",
    "- Run a preprocessing dataset command for the model\n",
    "- Run a training command\n",
    "- Run an inference command\n",
    "- Debug\n",
    "\n",
    "**Note**: If you're a beginner, much of the vernacular will be foregin to you as it was to me. it's best to not worry about details (difference between lora/qlora, how to evaluate models and datasets, etc.) for the first few iterations. Once you get a feel, come back and visit the notes section or embedding links in these blogs to get some context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c579897e-a6c5-4d8c-9afd-6dda38bc5190",
   "metadata": {},
   "source": [
    "## **Start**\n",
    "### **1. Selecting a Framework** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe1eeae-452a-425f-9fa4-51baeda06fed",
   "metadata": {},
   "source": [
    "I'll start by running my first fine tuning job with Axolotl. The Mastering LLM's conference bundle came with tons of compute (GPU-power) from various GPU providers, so I arbitrarily picked [jarvislabs.ai](https://jarvislabs.ai/). It's time to fine-tune!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d95554-75ab-4142-9eb1-d517034b7f71",
   "metadata": {},
   "source": [
    "Once inside JarvisLabs (after making an account, etc.), I clicked on Axolotl when given the option to select a framework for training. Then hit \"run on cloud\". Note that you may have to pay for compute on Jarvis if you don't already own some for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52481cce-1c71-41b8-a66d-6ef6c50b1bf0",
   "metadata": {},
   "source": [
    "### **2 & 3. Pick a Dataset and Model**\n",
    "Let's start by visiting the - [Axolotl GitHub Repo](https://github.com/OpenAccess-AI-Collective/axolotl) and navigating to the examples folder. This contains tons of models that have been run on Axolotl with various configuration (config) files listed in each model that have been used to run those models. Pick a model (try tiny-llama, for example, as it's better to start with smaller models for debugging purposes). Inside, you can see various configs. Click on lora.yml. Notice all of the parameters, like data paths, tokenization settings, and any specific preprocessing steps needed for the dataset. Yaml files specify model architecture, learning rates, batch sizes, and more.\n",
    "\n",
    "Examine the dataset in the yaml file. This example configuration lists the [alpaca_2k_test](https://huggingface.co/datasets/mhenrichsen/alpaca_2k_test) as the dataset used for training. Let's stick with this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c573235-5af1-48fc-9cd4-d2d571791f67",
   "metadata": {},
   "source": [
    "### **4 & 5. Pick a GPU and Launch**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb7df63-ec59-4c28-bb1f-1aac7cdf5742",
   "metadata": {},
   "source": [
    "Toggle back to jarvis. It's time to pick a GPU to train our model. This dictates how much CPU, RAM, and VRAM will be available to you when training. I picked the 1 x RTX6000Ada. Now, skip the rest and launch it! Select VSCode or Jupyter (whichever is more comfortable) to open the instance in the cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f10fc91-7290-4b2d-80d6-bb15bccdf8d8",
   "metadata": {},
   "source": [
    "### **6 & 7. Clone Axolotl and Create a Venv**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541d8568-b17c-478b-830c-323bf3febe54",
   "metadata": {},
   "source": [
    "First, clone the Axolotl repository and navigate inside it. Then, create your virtual environment, where we will begin to install dependencies. These package installs can easily cause dependency issues when running later commands, so it's important to keep track of which dependencies you install. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3ddb92-fceb-4ee9-bbf9-d4283cc54a2b",
   "metadata": {},
   "source": [
    "```sh\n",
    "git clone https://github.com/OpenAccess-AI-Collective/axolotl\n",
    "cd axolotl\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902746ae-ca50-424d-9c3b-e83791393423",
   "metadata": {},
   "source": [
    "```sh\n",
    "python3 -m venv axolotl-env\n",
    "source axoltol-env/bin/activate\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1131162b-c38e-47a9-b3d0-ee8bb49eff44",
   "metadata": {},
   "source": [
    "```sh\n",
    "pip3 install packaging\n",
    "pip3 install -e '.[flash-attn,deepspeed]'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75039fa-0be4-48ee-b5e2-f465b1436460",
   "metadata": {},
   "source": [
    "### **8. Authenticate to HF (if necessary)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26442fe7-b9cf-490d-9146-76728f27b24b",
   "metadata": {},
   "source": [
    "Sometimes, you need to accept permissions to use a dataset on HuggingFace. Login below and find your HuggingFace token at [HF Token](https://huggingface.co/settings/tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c690ab54-adb2-497e-9ca0-8af400eee7cb",
   "metadata": {},
   "source": [
    "```sh\n",
    "pip install huggingface_hub\n",
    "huggingface-cli login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077d8024-e6d9-466b-b926-b49423ae6bc9",
   "metadata": {},
   "source": [
    "### **9, 10, 11. Preprocess, Train, and Inference the Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcb7d5b-1851-462a-b5c7-0a073a9bd60c",
   "metadata": {},
   "source": [
    "These are where the money gets made! You can use any model in the following commands, but we'll use examples/tiny-llama/lora.yml."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ceef3c5-5742-4547-aba2-3e8651ec9fab",
   "metadata": {},
   "source": [
    "**Preprocessing Datasets (optional):**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca62a5d-32b0-4e39-a7da-a9ebe5bcc40e",
   "metadata": {},
   "source": [
    "```python\n",
    "python -m axolotl.cli.preprocess examples/tiny-llama/lora.yml.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f4dd34-09a1-48f4-9c6a-c8bcb13479d5",
   "metadata": {},
   "source": [
    "Runs the preprocess module from the axolotl.cli package on the config file specifying the dataset and preprocessing options. If you want to specify particular GPUs, set CUDA_VISIBLE_DEVICES to the IDs of those GPUs (e.g., 0 for the first GPU). If you want to use all available GPU's, don't include the command. If you set it as an empty string, it will only use the CPU (CUDA_VISIBLE_DEVICES=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240a0469-e264-40b3-9d33-6df9ceabba3b",
   "metadata": {},
   "source": [
    "Preprocess reads the yml file to understand how the dataset should be processed, and then tokenizes the raw text data into a format suitable for the model. Tokenizers sometimes are specific to the model itself. Here, any data augmentation specified in yml are applied to the dataset The preprocessed dataset is saved in an a ready-to-go format for the training step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f47663-283c-4760-81a2-d27ba868b736",
   "metadata": {},
   "source": [
    "**Fine Tune the Model:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40ef971-9805-4da4-9963-b63c603977f9",
   "metadata": {},
   "source": [
    "```python\n",
    "accelerate launch -m axolotl.cli.train examples/tiny-llama/lora.yml.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171a9384-43b0-4fa1-a26a-f69c8cadfa22",
   "metadata": {},
   "source": [
    "This accelerate launch command launches the training process using the accelerate library, which handles distributed training and optimizes usage of available hardware resources.\n",
    "\n",
    "-m specifies the module to run (train mod from axolotl.cli). We then pass the tiny llama model for training. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cda33f0-e7f4-4e51-9d73-c94aad06d22b",
   "metadata": {},
   "source": [
    "Here's a quick rundown of what is happening during the training loop:\n",
    "- Forward Pass: Data is passed through the model to compute predictions.\n",
    "- Loss Calculation: The difference between the predictions and the actual labels (targets) is computed as the loss.\n",
    "- Backward Pass: Gradients are computed using backpropagation to update the model weights.\n",
    "- Optimization Step: The optimizer adjusts the model weights based on the computed gradients.\n",
    "- Checkpointing: Periodically, the model state is saved (checkpoints) to allow for resuming training or for future inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3422cd4a-9409-400f-903d-ce6f764771b0",
   "metadata": {},
   "source": [
    "**Inference:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdd1fc9-ff93-4e8b-88d3-cdc3eb43eb29",
   "metadata": {},
   "source": [
    "```python\n",
    "accelerate launch -m axolotl.cli.inference examples/tiny-llama/lora.yml. \\\n",
    "    --lora_model_dir=\"./outputs/lora-out\" --gradio\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c41a5a0-ea95-41c8-8a8f-7a7741edbae6",
   "metadata": {},
   "source": [
    "We pass our model as an argument to the inference module from the axolotl.cli package.\n",
    "\n",
    "Option: \n",
    "```python\n",
    "--lora_model_dir=\"./outputs/lora-out\"\n",
    "```\n",
    "Explanation: This specifies the directory where the fine-tuned weights are stored. The inference process will load the model from this directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278b70cd-5f2e-4dde-968f-e806ee15446b",
   "metadata": {},
   "source": [
    "Under the hood overview:\n",
    "- Loading Model: The fine-tuned model, including the LoRA-specific weights, is loaded from the specified directory.\n",
    "- Inference Loop: The model processes the input data to generate predictions. This involves a forward pass and post processing.\n",
    "- Forward Pass: Input data is passed through the model to generate predictions.\n",
    "- Post-Processing: The raw output from the model is post-processed to convert it into a human-readable or application-specific format.\n",
    "- The predictions are outputted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4203d822-386b-49f6-9b34-0754354f80c6",
   "metadata": {},
   "source": [
    "### **12. Debugging**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb5f1a6-d0c6-4dc1-ab77-5bdd105c9309",
   "metadata": {},
   "source": [
    "While you'll want to change parameters in your yaml file in certain iterations to learn, sometimes, you'll want to change certain parameters to debug. Use these links for common ways to debug your yaml file and other tips:\n",
    "- [Hamel's Blog](https://hamel.dev/blog/posts/axolotl/) (Debugging)\n",
    "- [Axolotl Docs](https://openaccess-ai-collective.github.io/axolotl/docs/debugging.html#general-tips) (Debugging)\n",
    "\n",
    "Aside from that, here are some parameters I've changed throughout various runs that have saved me from errors:\n",
    "- use_reentrant: flip to False\n",
    "- flash_attention: flip to False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
