{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9095e23f-a5f5-4117-949f-e53bd224bc49",
   "metadata": {},
   "source": [
    "# What Model and Model Configuration (LoRA, etc.) should I Fine-Tune with?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3978950-6033-4067-bda7-c47796e77fd4",
   "metadata": {},
   "source": [
    "## Picking a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef549d34-8e04-4263-b5a8-0f1e9c90cd40",
   "metadata": {},
   "source": [
    "### Model size\n",
    "It depends on the use case, but if the breadth is relatively medium to narrow, there's no need to deal with the paralellism of much larger models. It's faster and easier to get the GPU that those run on\n",
    "-At time of writing, 7B parameter models are the most popular (not instruction tuned models). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54863329-932a-4671-869f-04335a44b1c2",
   "metadata": {},
   "source": [
    "### Model family\n",
    "\n",
    "Thanks to the way it's abstracted from axolotl, it's very easy to try different models (especially if they fit into the same GPU). Even if you have to use a new instance, it should be easy. \n",
    "\n",
    "- Do whatever is fashionable. Take the recently released llama 3 and use it! If it's recently released and widely used it's likely to be very good. \n",
    "\n",
    "- To do this, you can go to HuggingFace and sort by hotness. You can also go to community subreddits (r/LocalLLaMA). \n",
    "\n",
    "- People overindex on this. If you've run a couple of models, that should work fine. You won't improve it immensely by trying other models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34aee2e2-7bbf-4e94-a9ce-2478a6466555",
   "metadata": {},
   "source": [
    "### LoRA vs Full Fine Tune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693a9507-2c53-4455-a306-ca77b706ebe3",
   "metadata": {},
   "source": [
    "Almost always use LoRA or some version of it. As a practicioner, you will almost never need a full fine tune. There are only theoretical reasons to use it most often. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08a9536-80b5-49e7-ac8e-83b3f9b84225",
   "metadata": {},
   "source": [
    "## LoRA and QLoRA: Efficient Model Fine-Tuning Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa810673-3cb9-47c0-8f0e-f49321561be9",
   "metadata": {},
   "source": [
    "The vast majority of fine-tuning (that I know of) is done on LoRA and QLoRA. Let's take a look at both:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e344a6b4-4557-4033-a2bb-35d8078da703",
   "metadata": {},
   "source": [
    "### LoRA in a NutShell (Low-Rank Adaptation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f5f665-2a4d-4556-93c5-f5e7c66d6fad",
   "metadata": {},
   "source": [
    "**Key**:\n",
    "- **W** = Weight Matrix\n",
    "- **R** = Set of Real Numbers\n",
    "- **∈** = \"an element of\", \"belongs to\"\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Weight Matrix (W):** A matrix with dimensions representing model parameters\n",
    "- **Feed Forward Network:** Consists of layers transforming input embeddings into output vector representations, typically high-dimensional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a56e914-3b06-4248-a300-ddbfc62ca269",
   "metadata": {},
   "source": [
    "**Goal** Reduce number of parameters for efficient fine-tuning\n",
    "\n",
    "**Method:** Decompses the weight matrix into smaller matrices, reducing overall parameter count\n",
    "\n",
    "**Benefit:** Less memory and computational resources used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9457767f-c22c-4a23-a501-a0ba221a23ea",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0067d63-9744-49ab-bfc6-6fe10791bac5",
   "metadata": {},
   "source": [
    "Let's start to explain the LoRA approach with an example of a matrix with dimensions: \n",
    "\n",
    "- **Matrix W ∈ R^(4000×4000).** \n",
    "\n",
    "Fine-tuning this matrix on a feed forward network would require updating all 16 million parameters, which wouldn't be ideal for resources and memory use.\n",
    "\n",
    "LoRA approximates W with the product of two smaller matrices, which reducing the number of parameters needing to be learned and updated. Let's say LoRA reduces matrix W into Matrix A and Matrix B below:\n",
    "\n",
    "- **Matrix A ∈ R^(4000×16)**\n",
    "- **Matrix B ∈ R^(16×4000)**\n",
    "  \n",
    "Number of parameters for A and B (64k + 64k) = 128k parameters. The sum of A and B's parameters is far less than the product of our original matrix W's parameters (16 m). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69dd55a-63cd-4674-a787-79d3af2deb69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71af565a-63b3-448f-a108-d0dbec923c11",
   "metadata": {},
   "source": [
    "**Why LoRA works:**\n",
    "\n",
    "Any matrix W ∈ R^(m×n) can be decomposed into A ∈ R^(m×k) and B ∈ R^(k×n), where k is the rank of the approximation. If k is much smaller than m and n, the parameter count is greatly reduced. The rank of our example is 16.\n",
    "\n",
    "The sum of the parameters in A and B (128k) is the actual number of elements needing storage and updates during fine-tuning, not the product A × B.\n",
    "\n",
    "Matrix multiplication rules dictate that if  A ∈ R m×k and B ∈ R k×n, then the resulting matrix C= A×B will have the shape m×n. The inner dimensions (16 in this case) must match for the multiplication to be defined. Resulting matrix C will have shape mxn = 4k x 4k, which is the same shape as W. \n",
    "\n",
    "In other words, Total parameters involved in the LoRA method are indeed sum of params A and B (64k + 64k), which is different from resulting product A x B. This product gives shape 4k x 4k, but total number of elements (params) we need to store and learn during the fine tuning process is 64k + 64k. Difference of shape vs. number of parameters. \n",
    "\n",
    "**Benefits**\n",
    "\n",
    "With LoRA, we can allow the original model's parameters to remain mostly unchanged, capturing task-specific adaptations without full re-training. It's just a configuration flag. Since the model's parameters are mostly unchanged, it **reduces the risk of overfitting**.\n",
    "\n",
    "By using matrix factorization, LoRA allows for efficient adaptation of pre-trained models to new tasks with less GPU and RAM. It leverages matrix factorization to approximate large weight matrices with smaller, low-rank matrices.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5129eaab-8d59-4940-b14a-0528c9c8b56b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30dd4625-97cb-4003-a1e5-90a7f7e8e91e",
   "metadata": {},
   "source": [
    "### QLoRA: Quantized Low-Rank Adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6213edf0-6fc2-4942-9206-c8153bbfa4c6",
   "metadata": {},
   "source": [
    "**Adding Quantization to LoRA:**\n",
    "\n",
    "QLoRA reduces computational and memory requirements by furhter quantizing the low-rank matrices obtained from LoRA. It typically reduces from 32-bit floating point to 8 bit integers. \n",
    "\n",
    "For example, 64k elements at 32 bits = 2,048,000 bits, while 64k elements at 8 bits total 512,000 bits, achieving 4x memory savings. \n",
    "\n",
    "**Bit Reduction** divides the values for numbers into a smaller set of values (ex: 16 values for 4-bit storage). \n",
    "\n",
    "This is most suitable for resource-constrained environments. It might have some negative impact on your results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0a6d92-2984-4ca5-a7fc-539b84af9f04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ae16b2f-b811-44f4-a13d-c4e1ac9a2023",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e708a7-242e-40f9-969a-73d43de16f95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59a7662-1766-4a70-a478-66770f2d7b6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf2a00b-c0ed-443c-b1f9-d4318f3c7779",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43429d97-5218-494a-9d21-4a8fd2354931",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b5c22d-2a80-48bf-9061-1a988c361a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
